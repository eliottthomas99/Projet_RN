{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, device as torch_device, cuda, tanh\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16FeatureExtractor, self).__init__()\n",
    "\n",
    "        model = models.vgg16(pretrained=True)\n",
    "\n",
    "        # VGG-16 Feature Layers\n",
    "        self.features = nn.Sequential(*list(model.features))\n",
    "\n",
    "        # VGG-16 Average Pooling Layer\n",
    "        self.pooling = model.avgpool\n",
    "\n",
    "        # Convert the image into one-dimensional vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # First part of fully-connected layer from VGG16\n",
    "        self.fc = model.classifier[0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.pooling(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out) \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/lucavaio/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:15<00:00, 36.2MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG16FeatureExtractor(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (pooling): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=25088, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch_device('cuda:0' if cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VGG16FeatureExtractor().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, cnn, embed_dim):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        self.cnn = cnn  # init pretrained CNN\n",
    "        self.fc = nn.Linear(cnn.fc.out_features, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        # images shape: (8, 8, 2048)\n",
    "        features = self.cnn.forward(images)  # images shape: (64, 4096)\n",
    "        features = self.fc.forward(features)  # images shape: (64, 256)\n",
    "        features = F.relu(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "encoder = EncoderCNN(model, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, units):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.W1 = nn.Linear(encoder_dim, units)\n",
    "        self.W2 = nn.Linear(decoder_dim, units)\n",
    "        self.V = nn.Linear(units, 1)\n",
    "    \n",
    "    def forward(self, features, hidden_state):\n",
    "        # features shape: (batch, 64, 256)\n",
    "\n",
    "        # tanh scores\n",
    "        scores = tanh(\n",
    "            self.W1(features) + self.W2(hidden_state)\n",
    "        ) # (batch, 64, units)\n",
    "        scores = self.V(scores)  # (batch, 64, 1)\n",
    "        # scores = scores.squeeze(2)\n",
    "        \n",
    "        attention = F.softmax(scores, dim=1)  # (batch, 64)\n",
    "        \n",
    "        context_vector = features * attention.unsqueeze(2)  # (batch, 64, 256)\n",
    "        context_vector = context_vector.sum(dim=1)   # (batch, 64)\n",
    "        \n",
    "        return attention, context_vector"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53b6c1f84ea646dda3bc4674129caaad1b6b0ea52bb9674c24d9161dcc108757"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
