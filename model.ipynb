{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "NB_IMAGES = 8000\n",
    "\n",
    "embed_dim = 256\n",
    "units = 512\n",
    "num_steps = NB_IMAGES // BATCH_SIZE\n",
    "\n",
    "# Extraction from CNN: (64, 2048)\n",
    "features_shape = 2048\n",
    "attention_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        self.cnn = None  # init pretrained CNN\n",
    "        self.fc = nn.Dense(embed_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, images):\n",
    "        # images shape: (8, 8, 2048)\n",
    "        features = self.cnn.fit(images)  # images shape: (64, 2048)\n",
    "        features = self.fc.fit(features)  # images shape: (64, 256)\n",
    "        features = F.relu(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.W1 = nn.Linear(units)\n",
    "        self.W2 = nn.Linear(units)\n",
    "        self.V = nn.Linear(1)\n",
    "        \n",
    "    def forward(self, features, hidden_state):\n",
    "        # input shape: (64, embedding_shape)\n",
    "\n",
    "        # \"concat\" scores (tanh)\n",
    "        concat = torch.tanh(\n",
    "            self.W1(features) + self.W2(hidden_state)\n",
    "        ) # (64, units)\n",
    "        attention_scores = self.V(concat)  # (units, 1)\n",
    "        \n",
    "        alignment = F.softmax(attention_scores, dim=1)  # (units)\n",
    "        \n",
    "        context_vector = features * alignment.unsqueeze(2)  # (units, hidden_size)\n",
    "        context_vector = context_vector.sum(dim=1)   # (units)\n",
    "        \n",
    "        return alignment, context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, units, vocab_size, embed_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "        self.attention = Attention(self.units)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_shape)\n",
    "        self.gru = nn.GRU(units)\n",
    "        self.fc1 = nn.Linear(self.units)\n",
    "        self.fc2 = nn.Linear(vocab_size)\n",
    "    \n",
    "    def forward(self):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderRNN()\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder.forward(images)\n",
    "        outputs = self.decoder.forward(features, captions)\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53b6c1f84ea646dda3bc4674129caaad1b6b0ea52bb9674c24d9161dcc108757"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
